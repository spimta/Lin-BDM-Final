{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvnKTXzOO5p_"
   },
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D1Sn_NiL9Ls"
   },
   "source": [
    "we use Spark to explore [Safegraph data](https://www.safegraph.com/covid-19-data-consortium) to better understand how NYC response to the COVID-19 pandemic. Similarly, we will be looking at the [Core Places](https://docs.safegraph.com/v4.0/docs#section-core-places) and the [Weekly Pattern](https://docs.safegraph.com/v4.0/docs/places-schema#section-patterns) data sets to answer the following two inquiries:\n",
    "\n",
    "\n",
    "1.   How many restaurants in NYC were closed right when the city shut down on March 17, 2020, and how many were closed by April 1, 2020?\n",
    "\n",
    "2.   For those that were open on/after April 1, 2020 in [1], which ones still received a high volume of visits (in any day on/after April 1)? What were the median dwelling time at each  establishment in the first week of March (3/2-3/9) and in the first week of April (3/30-4/6)?\n",
    "\n",
    "### Definitions\n",
    "\n",
    "* *NYC*: a restaurant is considered to be in NYC if its city is `'New York'`, `'Brooklyn'`, `'Queens'`, `'Bronx'`, or `'Staten Island'`.\n",
    "\n",
    "* *Open*: a restaurant is considered open for a day if it has 1 or more visitors reported on that day in the *Weekly Pattern* data set.\n",
    "\n",
    "* *High Volume of Visits*: a restaurant is considered to receive a high volume of visitors if it has 50 or more visits on a day reported on that day in the *Weekly Pattern* data set.\n",
    "\n",
    "* *Median Dwelling Time*: though the *Weekly Pattern* report the median dwelling time in the **`median_dwell`** field, we would like to exclude those staying more than 4 hours (mostly employees) when calculating our median dwelling time. Thus, the *median dwelling time* should be computed from the **`bucketed_dwell_times`** without the **>240** bucket. The median dwelling time should only have one of the values `'<5'`, `'5-10'`, `'11-20'`, `'21-60'`, `'61-120'`, `'121-240'`, or `'N/A'` if it could not be determined.\n",
    "\n",
    " ## Requirements\n",
    "\n",
    "* You have to use Spark for this assignment.\n",
    "\n",
    "* Our data sets (`core_poi_ny.csv` and `nyc_restaurant_pattern.csv`) are assumed to be on HDFS, and could only be accessed using Spark (either as a Spark's DataFrame or an RDD).\n",
    "\n",
    "* You are not allowed to collect the raw data to the notebook and process them without using Spark. However, it is okay to collect intermediate data for processing. Just try to collect as little as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx1oVB3NgKXr"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOgEx3Csn1u9",
    "outputId": "14ce4a75-ba2c-4c0c-82d9-94c66560c806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZK8ql8arn0pkIJZIfknNscKXn85L9ZXX\n",
      "To: /content/core_poi_ny.csv\n",
      "100% 95.6M/95.6M [00:00<00:00, 103MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1NeXqsAeIJ8zukHt5cR2s19beDoz2Xw5d\n",
      "To: /content/nyc_restaurant_pattern.csv\n",
      "100% 101M/101M [00:00<00:00, 122MB/s]  \n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 48 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 81.3 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=19e2540601af83e1e2d88dc271ee574d699a57dba2771a2a460db17ecb0c3f3a\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1ZK8ql8arn0pkIJZIfknNscKXn85L9ZXX -O core_poi_ny.csv\n",
    "!gdown --id 1NeXqsAeIJ8zukHt5cR2s19beDoz2Xw5d -O nyc_restaurant_pattern.csv\n",
    "!pip install pyspark\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, IntegerType, MapType, StringType\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvhBqlchgR5K"
   },
   "source": [
    "## Task 1\n",
    "*Question: How many restaurants in NYC were closed right when the city shut down on March 17, 2020, and how many were closed by April 1, 2020?*\n",
    "\n",
    "The final output of Task 1 solution should be in the following form (with AAA and BBB being your actual computation):\n",
    "```\n",
    "The number of restaurants in NYC closed by March 17, 2020: 49\n",
    "The number of restaurants in NYC closed by April 01, 2020: 496\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "278rukIRjDXV"
   },
   "source": [
    "### Data preparation\n",
    "\n",
    "We will look at records of only 3 different restaurants in NYC. After this, we can treat **`dfPattern`** as the data frame pointing to the NYC restaurant pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Nnf8C8qjBSC",
    "outputId": "a43cefde-1a62-4e64-a316-f60f1434eeb6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------------+-------------------------+--------------------+--------------+--------+------+-----------+----------------+-------------------+------+--------------------+--------------------+----------------+------------------+--------------------+--------------------+------------+--------------------+--------------------+-------------------------+------------------+------------+--------------------+----------------------+-----------------------+--------------------+\n",
      "|           placekey|  safegraph_place_id|parent_placekey|parent_safegraph_place_id|       location_name|street_address|    city|region|postal_code|iso_country_code|safegraph_brand_ids|brands|    date_range_start|      date_range_end|raw_visit_counts|raw_visitor_counts|       visits_by_day| visits_by_each_hour|     poi_cbg|   visitor_home_cbgs|visitor_daytime_cbgs|visitor_country_of_origin|distance_from_home|median_dwell|bucketed_dwell_times|related_same_day_brand|related_same_week_brand|         device_type|\n",
      "+-------------------+--------------------+---------------+-------------------------+--------------------+--------------+--------+------+-----------+----------------+-------------------+------+--------------------+--------------------+----------------+------------------+--------------------+--------------------+------------+--------------------+--------------------+-------------------------+------------------+------------+--------------------+----------------------+-----------------------+--------------------+\n",
      "|229-223@627-s4r-2hq|sg:d3fdb6458c544b...|           null|                     null|All Stars Sports ...| 327 W 57th St|New York|    NY|      10019|              US|               null|  null|2020-03-02T00:00:...|2020-03-09T00:00:...|              25|                25|     [3,7,2,3,3,4,3]|[0,0,0,0,0,0,0,0,...|360610139007|{\"360610135002\":4...|{\"360610129002\":4...|                {\"US\":22}|             10591|        29.0|{\"<5\":2,\"5-10\":5,...|  {\"Morgenthal Fred...|   {\"Walgreens\":21,\"...|{\"android\":15,\"io...|\n",
      "|22c-224@627-s8h-y7q|sg:2cbad77e421c4c...|           null|                     null|Hole in The Wall ...|  37 W 24th St|New York|    NY|      10010|              US|               null|  null|2020-03-02T00:00:...|2020-03-09T00:00:...|               8|                 8|     [1,2,0,1,3,0,1]|[0,0,0,0,0,0,0,0,...|360610058001|  {\"391034170004\":4}|  {\"360050365011\":4}|                 {\"US\":7}|             19346|        55.5|{\"<5\":1,\"5-10\":1,...|  {\"McDonald's\":96,...|   {\"Dunkin'\":48,\"Sh...|{\"android\":0,\"ios...|\n",
      "|225-222@627-s8j-rhq|sg:0e86fc3cfbc141...|           null|                     null|     Cafe Deli cious|   491 1st Ave|New York|    NY|      10016|              US|               null|  null|2020-03-02T00:00:...|2020-03-09T00:00:...|             620|               349|[114,112,76,106,9...|[4,2,3,0,0,1,7,8,...|360610066009|{\"360610062001\":8...|{\"360610062001\":2...|               {\"US\":334}|             11162|        66.5|{\"<5\":17,\"5-10\":1...|                    {}|   {\"Dunkin'\":20,\"St...|{\"android\":239,\"i...|\n",
      "|229-223@627-s4r-2hq|sg:d3fdb6458c544b...|           null|                     null|All Stars Sports ...| 327 W 57th St|New York|    NY|      10019|              US|               null|  null|2020-03-09T00:00:...|2020-03-16T00:00:...|              23|                20|     [0,7,5,3,4,2,2]|[0,0,0,0,0,0,0,0,...|360610139007|{\"360610135001\":4...|{\"530090014001\":4...|                {\"US\":16}|             20453|        42.0|{\"<5\":1,\"5-10\":5,...|  {\"Food Universe M...|   {\"Starbucks\":30,\"...|{\"android\":11,\"io...|\n",
      "|22c-224@627-s8h-y7q|sg:2cbad77e421c4c...|           null|                     null|Hole in The Wall ...|  37 W 24th St|New York|    NY|      10010|              US|               null|  null|2020-03-09T00:00:...|2020-03-16T00:00:...|               4|                 4|     [2,0,0,1,1,0,0]|[0,0,0,0,0,0,0,0,...|360610058001|                  {}|                  {}|                 {\"US\":4}|              null|        45.0|{\"<5\":0,\"5-10\":0,...|                    {}|                     {}|{\"android\":0,\"ios...|\n",
      "|225-222@627-s8j-rhq|sg:0e86fc3cfbc141...|           null|                     null|     Cafe Deli cious|   491 1st Ave|New York|    NY|      10016|              US|               null|  null|2020-03-09T00:00:...|2020-03-16T00:00:...|             607|               335|[101,116,106,100,...|[2,1,2,1,0,0,4,13...|360610066009|{\"360610062001\":9...|{\"360610062001\":2...|               {\"US\":311}|             11051|        45.0|{\"<5\":23,\"5-10\":1...|         {\"Dunkin'\":5}|   {\"Starbucks\":17,\"...|{\"android\":231,\"i...|\n",
      "|229-223@627-s4r-2hq|sg:d3fdb6458c544b...|           null|                     null|All Stars Sports ...| 327 W 57th St|New York|    NY|      10019|              US|               null|  null|2020-03-16T00:00:...|2020-03-23T00:00:...|              13|                12|     [3,2,2,1,3,2,0]|[0,0,0,0,0,0,0,0,...|360610139007|  {\"360594166002\":4}|{\"360610135002\":4...|                {\"US\":14}|             13375|        55.0|{\"<5\":1,\"5-10\":4,...|  {\"Duane Reade by ...|   {\"Subway\":14,\"CVS...|{\"android\":9,\"ios...|\n",
      "|22c-224@627-s8h-y7q|sg:2cbad77e421c4c...|           null|                     null|Hole in The Wall ...|  37 W 24th St|New York|    NY|      10010|              US|               null|  null|2020-03-16T00:00:...|2020-03-23T00:00:...|               1|                 1|     [1,0,0,0,0,0,0]|[0,0,0,0,0,0,0,0,...|360610058001|                  {}|                  {}|                 {\"US\":4}|              null|        55.0|{\"<5\":0,\"5-10\":0,...|                    {}|                     {}|{\"android\":0,\"ios...|\n",
      "|225-222@627-s8j-rhq|sg:0e86fc3cfbc141...|           null|                     null|     Cafe Deli cious|   491 1st Ave|New York|    NY|      10016|              US|               null|  null|2020-03-16T00:00:...|2020-03-23T00:00:...|             485|               228|[69,74,97,104,71,...|[0,2,1,0,0,0,5,6,...|360610066009|{\"360594163001\":8...|{\"360610062001\":2...|               {\"US\":212}|             12344|        66.0|{\"<5\":13,\"5-10\":9...|                    {}|   {\"Dunkin'\":17,\"St...|{\"android\":156,\"i...|\n",
      "|229-223@627-s4r-2hq|sg:d3fdb6458c544b...|           null|                     null|All Stars Sports ...| 327 W 57th St|New York|    NY|      10019|              US|               null|  null|2020-03-23T00:00:...|2020-03-30T00:00:...|               9|                 9|     [1,2,3,2,1,0,0]|[0,0,0,0,0,0,0,0,...|360610139007|  {\"360610236001\":4}|{\"360610139006\":7...|                 {\"US\":8}|             12751|        21.0|{\"<5\":1,\"5-10\":2,...|  {\"Zara\":100,\"Pret...|   {\"BareBurger\":33,...|{\"android\":4,\"ios...|\n",
      "|225-222@627-s8j-rhq|sg:0e86fc3cfbc141...|           null|                     null|     Cafe Deli cious|   491 1st Ave|New York|    NY|      10016|              US|               null|  null|2020-03-23T00:00:...|2020-03-30T00:00:...|             388|               192|[62,78,73,66,52,3...|[3,1,1,1,0,2,4,9,...|360610066009|{\"360610066009\":1...|{\"360610062001\":1...|               {\"US\":182}|             11706|        46.0|{\"<5\":11,\"5-10\":7...|         {\"Wendy's\":6}|   {\"Dunkin'\":11,\"St...|{\"android\":134,\"i...|\n",
      "|229-223@627-s4r-2hq|sg:d3fdb6458c544b...|           null|                     null|All Stars Sports ...| 327 W 57th St|New York|    NY|      10019|              US|               null|  null|2020-03-30T00:00:...|2020-04-06T00:00:...|               2|                 2|     [1,1,0,0,0,0,0]|[0,0,0,0,0,0,1,0,...|360610139007|                  {}|                  {}|                       {}|              null|        24.5|{\"<5\":0,\"5-10\":1,...|                    {}|                     {}|{\"android\":4,\"ios...|\n",
      "|225-222@627-s8j-rhq|sg:0e86fc3cfbc141...|           null|                     null|     Cafe Deli cious|   491 1st Ave|New York|    NY|      10016|              US|               null|  null|2020-03-30T00:00:...|2020-04-06T00:00:...|             382|               186|[60,70,57,68,46,4...|[1,1,1,2,0,1,3,12...|360610066009|{\"360610062001\":1...|{\"360610062001\":1...|               {\"US\":156}|             12495|        42.0|{\"<5\":11,\"5-10\":7...|  {\"Target\":7,\"Star...|   {\"Dunkin'\":12,\"St...|{\"android\":134,\"i...|\n",
      "+-------------------+--------------------+---------------+-------------------------+--------------------+--------------+--------+------+-----------+----------------+-------------------+------+--------------------+--------------------+----------------+------------------+--------------------+--------------------+------------+--------------------+--------------------+-------------------------+------------------+------------+--------------------+----------------------+-----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RESTAURANTS = set(['sg:2cbad77e421c4ccb8ffd20d2a6b81f78',\n",
    "                   'sg:d3fdb6458c544bb687d2da3eb1c8e28e',\n",
    "                   'sg:0e86fc3cfbc1417fab6a5ef1b4a63026'])\n",
    "\n",
    "dfPattern = spark.read.csv('nyc_restaurant_pattern.csv', \n",
    "                           header=True, escape='\"') \\\n",
    "    .where(F.col('safegraph_place_id').isin(RESTAURANTS))\n",
    "\n",
    "dfPattern.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcJgwfFuirqN"
   },
   "source": [
    "### STRATEGY:\n",
    "\n",
    "Our strategy in using DataFrame would be different than using RDD. To best utilizing the functionalitie of DataFrame, we should with structured data as much as possible. This means that we should keep our data to be in the table whenever we can and working on columns separately. With that in mind, instead of sum up the visits from a date, e.g. March 17, onwards, we could exploding the **`visits_by_day`** column into **`date`** and **`visits`**, then performing our filtering from there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM0BH_umqAeN"
   },
   "source": [
    "### A. Keep only `safegraph_place_id`, `date_range_start`, and `visits_by_day` for each record.\n",
    "\n",
    "**NOTE**: we do not need `date_range_end` in this task since we will be exploding all 7 days of visits starting from `date_range_start`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1zZlGTWAqAeZ",
    "outputId": "9f01c3b7-194c-43c6-b2d6-0851a12f77ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|  safegraph_place_id|    date_range_start|       visits_by_day|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|sg:d3fdb6458c544b...|2020-03-02T00:00:...|     [3,7,2,3,3,4,3]|\n",
      "|sg:2cbad77e421c4c...|2020-03-02T00:00:...|     [1,2,0,1,3,0,1]|\n",
      "|sg:0e86fc3cfbc141...|2020-03-02T00:00:...|[114,112,76,106,9...|\n",
      "|sg:d3fdb6458c544b...|2020-03-09T00:00:...|     [0,7,5,3,4,2,2]|\n",
      "|sg:2cbad77e421c4c...|2020-03-09T00:00:...|     [2,0,0,1,1,0,0]|\n",
      "|sg:0e86fc3cfbc141...|2020-03-09T00:00:...|[101,116,106,100,...|\n",
      "|sg:d3fdb6458c544b...|2020-03-16T00:00:...|     [3,2,2,1,3,2,0]|\n",
      "|sg:2cbad77e421c4c...|2020-03-16T00:00:...|     [1,0,0,0,0,0,0]|\n",
      "|sg:0e86fc3cfbc141...|2020-03-16T00:00:...|[69,74,97,104,71,...|\n",
      "|sg:d3fdb6458c544b...|2020-03-23T00:00:...|     [1,2,3,2,1,0,0]|\n",
      "|sg:0e86fc3cfbc141...|2020-03-23T00:00:...|[62,78,73,66,52,3...|\n",
      "|sg:d3fdb6458c544b...|2020-03-30T00:00:...|     [1,1,0,0,0,0,0]|\n",
      "|sg:0e86fc3cfbc141...|2020-03-30T00:00:...|[60,70,57,68,46,4...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfA = dfPattern.select('safegraph_place_id', 'date_range_start', 'visits_by_day')\n",
    "dfA.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMYsWI1psfQY"
   },
   "source": [
    "### B. Complete the `expandVisits()` function below to explode the `visits_by_day` column in `dfA` into 7 rows consisting of the date and the visit counts for that date starting from `date_range_start`.\n",
    "\n",
    "We will name the new columns as `date` and `visits`; and drop the existing `date_range_starts` and `visits_by_day`. In order to do this, we will have to write a User-defined Function (UDF) that takes 2 parameter (`date_range_starts` and `visits_by_date`) and return a **MapType()** of 7 entries with the keys are dates, and the values are visits for those dates.\n",
    "\n",
    "**NOTE:** we also truncate the datetime string of the visits to only keep the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAd_2ypEsfQk",
    "outputId": "2d913abb-e9ce-40df-88ef-2b04c7c163cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+\n",
      "|  safegraph_place_id|      date|visits|\n",
      "+--------------------+----------+------+\n",
      "|sg:d3fdb6458c544b...|2020-03-02|     3|\n",
      "|sg:d3fdb6458c544b...|2020-03-03|     7|\n",
      "|sg:d3fdb6458c544b...|2020-03-04|     2|\n",
      "|sg:d3fdb6458c544b...|2020-03-05|     3|\n",
      "|sg:d3fdb6458c544b...|2020-03-06|     3|\n",
      "|sg:d3fdb6458c544b...|2020-03-07|     4|\n",
      "|sg:d3fdb6458c544b...|2020-03-08|     3|\n",
      "|sg:2cbad77e421c4c...|2020-03-02|     1|\n",
      "|sg:2cbad77e421c4c...|2020-03-03|     2|\n",
      "|sg:2cbad77e421c4c...|2020-03-04|     0|\n",
      "|sg:2cbad77e421c4c...|2020-03-05|     1|\n",
      "|sg:2cbad77e421c4c...|2020-03-06|     3|\n",
      "|sg:2cbad77e421c4c...|2020-03-07|     0|\n",
      "|sg:2cbad77e421c4c...|2020-03-08|     1|\n",
      "|sg:0e86fc3cfbc141...|2020-03-02|   114|\n",
      "|sg:0e86fc3cfbc141...|2020-03-03|   112|\n",
      "|sg:0e86fc3cfbc141...|2020-03-04|    76|\n",
      "|sg:0e86fc3cfbc141...|2020-03-05|   106|\n",
      "|sg:0e86fc3cfbc141...|2020-03-06|    97|\n",
      "|sg:0e86fc3cfbc141...|2020-03-07|    59|\n",
      "+--------------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def expandVisits(date_range_start, visits_by_day):\n",
    "    '''\n",
    "    This function needs to return a Python's dict{datetime:int} where:\n",
    "      key   : datetime type, e.g. datetime(2020,3,17), etc.\n",
    "      value : the number of visits for that day\n",
    "    '''\n",
    "    date = date_range_start.split('T')[0].split('-')\n",
    "    year = int(date[0])\n",
    "    month = int(date[1])\n",
    "    day = int(date[2])\n",
    "    vbd = visits_by_day[1:-1].split(',')\n",
    "    mydict = dict()\n",
    "\n",
    "    week_day_count = 0\n",
    "    for i in vbd:\n",
    "      mydict[datetime.date(year, month, day)+datetime.timedelta(days=week_day_count)] = int(i)\n",
    "      week_day_count += 1\n",
    "    \n",
    "    return mydict\n",
    "    \n",
    "\n",
    "udfExpand = F.udf(expandVisits, MapType(DateType(), IntegerType()))\n",
    "dfB = dfA.select('safegraph_place_id',\n",
    "                 F.explode(udfExpand('date_range_start', 'visits_by_day')) \\\n",
    "                    .alias('date', 'visits'))\n",
    "\n",
    "dfB.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpGXVQZ_t_h3"
   },
   "source": [
    "### C. Next we drop all records in `dfB` with a `date` before March 17.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DKttCBM0lRe",
    "outputId": "ca4c54f5-a4af-4999-a879-1534b6ae609c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+\n",
      "|  safegraph_place_id|      date|visits|\n",
      "+--------------------+----------+------+\n",
      "|sg:d3fdb6458c544b...|2020-03-17|     2|\n",
      "|sg:d3fdb6458c544b...|2020-03-18|     2|\n",
      "|sg:d3fdb6458c544b...|2020-03-19|     1|\n",
      "|sg:d3fdb6458c544b...|2020-03-20|     3|\n",
      "|sg:d3fdb6458c544b...|2020-03-21|     2|\n",
      "|sg:d3fdb6458c544b...|2020-03-22|     0|\n",
      "|sg:2cbad77e421c4c...|2020-03-17|     0|\n",
      "|sg:2cbad77e421c4c...|2020-03-18|     0|\n",
      "|sg:2cbad77e421c4c...|2020-03-19|     0|\n",
      "|sg:2cbad77e421c4c...|2020-03-20|     0|\n",
      "|sg:2cbad77e421c4c...|2020-03-21|     0|\n",
      "|sg:2cbad77e421c4c...|2020-03-22|     0|\n",
      "|sg:0e86fc3cfbc141...|2020-03-17|    74|\n",
      "|sg:0e86fc3cfbc141...|2020-03-18|    97|\n",
      "|sg:0e86fc3cfbc141...|2020-03-19|   104|\n",
      "|sg:0e86fc3cfbc141...|2020-03-20|    71|\n",
      "|sg:0e86fc3cfbc141...|2020-03-21|    37|\n",
      "|sg:0e86fc3cfbc141...|2020-03-22|    33|\n",
      "|sg:d3fdb6458c544b...|2020-03-23|     1|\n",
      "|sg:d3fdb6458c544b...|2020-03-24|     2|\n",
      "+--------------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfC = dfB.filter(dfB[\"date\"] >= datetime.date(2020,3,17))\n",
    "dfC.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7qD5Q9Uwldg"
   },
   "source": [
    "### D. Find the max number of visits for each restaurant. This is similar to a word count, and can be done \"reduce\" all records by restaurant ID with a `max` operator. Name this new column (of the max visits) as `max_visits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Mm5eLb2t_iC",
    "outputId": "dc07e95b-a9ed-496a-ccc6-18981a0b0e51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|  safegraph_place_id|max_visits|\n",
      "+--------------------+----------+\n",
      "|sg:d3fdb6458c544b...|         3|\n",
      "|sg:2cbad77e421c4c...|         0|\n",
      "|sg:0e86fc3cfbc141...|       104|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfD = dfC.groupBy('safegraph_place_id') \\\n",
    "      .max(\"visits\").withColumnRenamed(\"max(visits)\", \"max_visits\")\n",
    "dfD.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVQO-Ljrxttm"
   },
   "source": [
    "### E. Finally, we count the number of records in `dfD` that has 0 visits, i.e. the maximum number of visits starting from March 17 is 0. This should be our final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxNGKaNoxtty",
    "outputId": "0697314f-5f8b-4542-a909-ed264dfbb5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|  safegraph_place_id|max_visits|\n",
      "+--------------------+----------+\n",
      "|sg:2cbad77e421c4c...|         0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfE = dfD.filter(dfD.max_visits == 0)\n",
    "dfE.show()\n",
    "dfE.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYUoI6xT1vqM"
   },
   "source": [
    "### F. Putting it all together, replace `dfPattern` with the full data set and rerun your code from the above steps to get the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dEbOxbRcyNR0",
    "outputId": "d03feef0-6094-4d46-9590-8c3aa509a50a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of restaurants in NYC closed by March 17, 2020: 49\n"
     ]
    }
   ],
   "source": [
    "dfPattern = spark.read.csv('nyc_restaurant_pattern.csv', \n",
    "                           header=True, escape='\"')\n",
    "\n",
    "dfA = dfPattern.select('safegraph_place_id', 'date_range_start', 'visits_by_day')\n",
    "\n",
    "def expandVisits(date_range_start, visits_by_day):\n",
    "    '''\n",
    "    This function needs to return a Python's dict{datetime:int} where:\n",
    "      key   : datetime type, e.g. datetime(2020,3,17), etc.\n",
    "      value : the number of visits for that day\n",
    "    '''\n",
    "    date = date_range_start.split('T')[0].split('-')\n",
    "    year = int(date[0])\n",
    "    month = int(date[1])\n",
    "    day = int(date[2])\n",
    "    vbd = visits_by_day[1:-1].split(',')\n",
    "    mydict = dict()\n",
    "\n",
    "    week_day_count = 0\n",
    "    for i in vbd:\n",
    "      mydict[datetime.date(year, month, day)+datetime.timedelta(days=week_day_count)] = int(i)\n",
    "      week_day_count += 1\n",
    "    \n",
    "    return mydict\n",
    "    \n",
    "\n",
    "udfExpand = F.udf(expandVisits, MapType(DateType(), IntegerType()))\n",
    "dfB = dfA.select('safegraph_place_id',\n",
    "                 F.explode(udfExpand('date_range_start', 'visits_by_day')) \\\n",
    "                    .alias('date', 'visits'))\n",
    "\n",
    "dfC = dfB.filter(dfB[\"date\"] >= datetime.date(2020,3,17))\n",
    "dfD = dfC.groupBy('safegraph_place_id') \\\n",
    "      .max(\"visits\").withColumnRenamed(\"max(visits)\", \"max_visits\")\n",
    "dfE = dfD.filter(dfD.max_visits == 0)\n",
    "print(\"The number of restaurants in NYC closed by March 17, 2020:\",dfE.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhpFmfi8mApp"
   },
   "source": [
    "## Task 2\n",
    "*Question: For those that were open on/after April 1, 2020 in [1], which ones still received a high volume of visits (in any day on/after April 1)? What were the median dwelling time at each establishment in the first week of March (3/2-3/9) and in the first week of April (3/30-4/6)?*\n",
    "\n",
    "The final output of Task 2 should be a CSV-like format (each establishment per line) **sorted alphabetically by Restaurant_Name**:\n",
    "```\n",
    "Restaurant_Name,Street_Address,City,Median_Dwell_Bucket_March,Median_Dwell_Bucket_April\n",
    "```\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "3 In 1 Kitchen,4902 Fort Hamilton Pkwy,Brooklyn,21-60,21-60\n",
    "Agape Cafe,655 W 34th St,New York,21-60,21-60\n",
    "Buffalo Wild Wings,632 Gateway Dr,Brooklyn,21-60,11-20\n",
    "Burger King,2800 Hylan Blvd,Staten Island,11-20,11-20\n",
    "Cafe Deli cious,491 1st Ave,New York,21-60,21-60\n",
    "Cinnabon,1313 Broadway,New York,11-20,11-20\n",
    "Dunkin',150 B Greaves Laneevergreen Plaza,New York,11-20,21-60\n",
    "Dunkin',1752 Shore Pkwybjs Wholesale Club,New York,21-60,21-60\n",
    "Dunkin',2449 Veterans Rd Wshop Rite,New York,21-60,21-60\n",
    "Dunkin',590 Gateway Dr,Brooklyn,21-60,21-60\n",
    "Dunkin',625 Atlantic Aveatlantic Center Mall,New York,21-60,21-60\n",
    "Dunkin',6620 Avenue U,Brooklyn,11-20,11-20\n",
    "Food Express Truck,2501 Forest Ave Across From Home Depot,New York,11-20,21-60\n",
    "Golden Krust Caribbean Bakery and Grill,1364 Pennsylvania Ave,Brooklyn,11-20,11-20\n",
    "Harlem Tavern,2153 Frederick Douglass Blvd,New York,11-20,11-20\n",
    "Hutong,731 Lexington Ave,New York,21-60,21-60\n",
    "Khan Express,1275 York Ave,New York,21-60,21-60\n",
    "King Cab Halal Food,10th Ave & 28th St,New York,N/A,21-60\n",
    "McDonald's,1403 Mermaid Ave,Brooklyn,5-10,5-10\n",
    "McDonald's,1600 Bruckner Blvd,Bronx,5-10,11-20\n",
    "McDonald's,2154 Hylan Blvd,Staten Island,11-20,5-10\n",
    "McDonald's,260 Page Ave,Staten Island,11-20,5-10\n",
    "Ninja 86 Sushi,2274 86th St,Brooklyn,11-20,11-20\n",
    "PROOF Coffee Roasters,335 E 27th St,New York,21-60,21-60\n",
    "Pizza Gusta,2945 Bruckner Blvd,Bronx,11-20,21-60\n",
    "Plaza Cafeteria Mount Sinai Hospital,1428 Madison Ave,New York,21-60,21-60\n",
    "Red Mango,234 W 34th St,New York,11-20,11-20\n",
    "Roti R Us,1493 Albany Ave,Brooklyn,21-60,21-60\n",
    "Starbucks,655 W 34th St,New York,11-20,21-60\n",
    "The Dumplin Shop,3852 Bronxwood Ave,Bronx,21-60,21-60\n",
    "```\n",
    "\n",
    "If there is no dwelling time information for a particular week, please report as **`'N/A'`**.\n",
    "\n",
    "Please note that you can find *Street Address* and *City* information from the *Core Places* data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jGsxh7J7bp0"
   },
   "source": [
    "### G. Similar to what we did in (E) but instead of counting those with 0 visits we will filter `dfD` to keep restaurants that have the maximum number of 50 visits or more. Unlike (E), we will keep the following fields for later usage in the output: `location_name`, `street_address`, and `city`.\n",
    "\n",
    "We can use the same `udfExpand` from (B) but data must be filtered to be from April 1, instead of from March 17. Note that, we cannot reuse `dfA` or `dfB` since those do not include the 3 new columns. We should start from `dfPattern` again.\n",
    "\n",
    "After this, we can drop the column `max_visits` since it will not be used any more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kb7tnpSY7bp3",
    "outputId": "1d83a5b9-eb81-4b03-fe1d-bf1b9abadfbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------+---------------+--------+\n",
      "|  safegraph_place_id|max_visits|street_address|  location_name|    city|\n",
      "+--------------------+----------+--------------+---------------+--------+\n",
      "|sg:0e86fc3cfbc141...|        68|   491 1st Ave|Cafe Deli cious|New York|\n",
      "+--------------------+----------+--------------+---------------+--------+\n",
      "\n",
      "+--------------------+--------------+---------------+--------+\n",
      "|  safegraph_place_id|street_address|  location_name|    city|\n",
      "+--------------------+--------------+---------------+--------+\n",
      "|sg:0e86fc3cfbc141...|   491 1st Ave|Cafe Deli cious|New York|\n",
      "+--------------------+--------------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfPattern = spark.read.csv('nyc_restaurant_pattern.csv', \n",
    "                           header=True, escape='\"') \\\n",
    "    .where(F.col('safegraph_place_id').isin(RESTAURANTS))\n",
    "\n",
    "dfG = dfPattern.select('safegraph_place_id', 'date_range_start', 'visits_by_day', 'location_name', 'street_address', 'city')\n",
    "\n",
    "\n",
    "def expandVisits(date_range_start, visits_by_day):\n",
    "    '''\n",
    "    This function needs to return a Python's dict{datetime:int} where:\n",
    "      key   : datetime type, e.g. datetime(2020,3,17), etc.\n",
    "      value : the number of visits for that day\n",
    "    '''\n",
    "    date = date_range_start.split('T')[0].split('-')\n",
    "    year = int(date[0])\n",
    "    month = int(date[1])\n",
    "    day = int(date[2])\n",
    "    vbd = visits_by_day[1:-1].split(',')\n",
    "    mydict = dict()\n",
    "\n",
    "    week_day_count = 0\n",
    "    for i in vbd:\n",
    "      mydict[datetime.date(year, month, day)+datetime.timedelta(days=week_day_count)] = int(i)\n",
    "      week_day_count += 1\n",
    "    \n",
    "    return mydict\n",
    "    \n",
    "\n",
    "udfExpand = F.udf(expandVisits, MapType(DateType(), IntegerType()))\n",
    "dfG = dfG.select('safegraph_place_id',\n",
    "                 F.explode(udfExpand('date_range_start', 'visits_by_day')) \\\n",
    "                    .alias('date', 'visits'), \\\n",
    "                 'location_name', 'street_address', 'city')\n",
    "\n",
    "dfG = dfG.filter(dfG[\"date\"] >= datetime.date(2020,4,1))\n",
    "\n",
    "dfG = dfG.groupBy('safegraph_place_id').agg({'visits':'max', 'location_name':'first', 'street_address':'first', 'city':'first'}) \\\n",
    "        .withColumnRenamed('max(visits)', 'max_visits') \\\n",
    "        .withColumnRenamed('first(location_name)', 'location_name') \\\n",
    "        .withColumnRenamed('first(street_address)', 'street_address') \\\n",
    "        .withColumnRenamed('first(city)', 'city')\n",
    "\n",
    "      \n",
    "dfG = dfG.filter(dfG.max_visits >= 50)\n",
    "dfG.show()\n",
    "\n",
    "dfG = dfG.drop('max_visits')\n",
    "dfG.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlN2bRMc-SxN"
   },
   "source": [
    "### H. On the other hand, we also need to filter the pattern data set to keep only records for the first week of March, and April, so that we can compute the median dwelling time.\n",
    "\n",
    "Complete the transformation below to include `safegraph_place_id`, `date`, and `bucketed_dwell_times` columns, where `date` is derived from `date_range_start` to keep only the date if it is either `'2020-03-02'` or `'2020-03-30'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83LDtj3k-SxZ",
    "outputId": "14f17a30-4f03-4024-9096-3274c1becec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|  safegraph_place_id|      date|bucketed_dwell_times|\n",
      "+--------------------+----------+--------------------+\n",
      "|sg:d3fdb6458c544b...|2020-03-02|{\"<5\":2,\"5-10\":5,...|\n",
      "|sg:2cbad77e421c4c...|2020-03-02|{\"<5\":1,\"5-10\":1,...|\n",
      "|sg:0e86fc3cfbc141...|2020-03-02|{\"<5\":17,\"5-10\":1...|\n",
      "|sg:d3fdb6458c544b...|2020-03-30|{\"<5\":0,\"5-10\":1,...|\n",
      "|sg:0e86fc3cfbc141...|2020-03-30|{\"<5\":11,\"5-10\":7...|\n",
      "+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfH = dfPattern.select('safegraph_place_id', 'date_range_start', 'bucketed_dwell_times')\n",
    "dfH = dfH.withColumn(\"date_range_start\", F.split(dfH.date_range_start, 'T').getItem(0)).withColumnRenamed('date_range_start', 'date')\n",
    "dfH = dfH.filter((dfH.date == '2020-03-02') | (dfH.date == '2020-03-30'))\n",
    "dfH = dfH.withColumn(\"date\", F.to_date(dfH.date, 'yyyy-MM-dd'))\n",
    "\n",
    "dfH.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0qrWwDV_7NI"
   },
   "source": [
    "### I. Complete the `medianDwellBucket` function below which takes a `bucketed_dwell_time` string similar to those in `dfH`, and return the median bucket label after removing `\">240\"` bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AI8v7htG_7NL",
    "outputId": "c6550826-1a0c-47ab-bba9-61169e853e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-20\n",
      "21-60\n"
     ]
    }
   ],
   "source": [
    "testI1 = '{\"<5\":11,\"5-10\":70,\"11-20\":68,\"21-60\":65,\"61-120\":36,\"121-240\":37,\">240\":101}'\n",
    "testI2 = '{\"<5\":1,\"5-10\":2,\"11-20\":1,\"21-60\":3,\"61-120\":1,\"121-240\":0,\">240\":1}'\n",
    "\n",
    "def medianDwellBucket(dwells):\n",
    "  dwells = json.loads(dwells)\n",
    "  del dwells['>240']\n",
    "  total = sum(dwells.values())\n",
    "  middle = total/2\n",
    "  tmp = 0\n",
    "  for key in dwells:\n",
    "    tmp = tmp + dwells[key]\n",
    "\n",
    "    if tmp > middle:\n",
    "      return key\n",
    "\n",
    "\n",
    "print(medianDwellBucket(testI1)) # should be '11-20'\n",
    "print(medianDwellBucket(testI2)) # should be '21-60'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFYNkPVpB0mW"
   },
   "source": [
    "### J. Make use of the `medianDwellBucket()` function above, we can transform the `bucketed_median_dwell` in `dfH` to the median bucket label, named `median_dwell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05VRKNUpB0mZ",
    "outputId": "94980680-f8c7-4e30-dc2d-7e7639f4a3f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+\n",
      "|  safegraph_place_id|      date|median_dwell|\n",
      "+--------------------+----------+------------+\n",
      "|sg:d3fdb6458c544b...|2020-03-02|       21-60|\n",
      "|sg:2cbad77e421c4c...|2020-03-02|       21-60|\n",
      "|sg:0e86fc3cfbc141...|2020-03-02|       21-60|\n",
      "|sg:d3fdb6458c544b...|2020-03-30|       21-60|\n",
      "|sg:0e86fc3cfbc141...|2020-03-30|       21-60|\n",
      "+--------------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfMedian = F.udf(lambda x: medianDwellBucket(x), StringType())\n",
    "\n",
    "dfJ = dfH.withColumn('median_dwell', udfMedian('bucketed_dwell_times')) \\\n",
    "    .select('safegraph_place_id', 'date', 'median_dwell')\n",
    "dfJ.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l_LRg9PWLuK"
   },
   "source": [
    "### K. We are going to join `dfJ` with `dfG` (the list of restaurants with at least 50 visits a day).\n",
    "\n",
    "For each restaurant, we should mostly have two records, one for the week of `'2020-03-02'`, and another for the week of `'2020-03-30'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3X-SL9KWLuW",
    "outputId": "16649cb8-3bad-4e5e-9354-09501bae5f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+--------------+---------------+--------+\n",
      "|  safegraph_place_id|      date|median_dwell|street_address|  location_name|    city|\n",
      "+--------------------+----------+------------+--------------+---------------+--------+\n",
      "|sg:0e86fc3cfbc141...|2020-03-30|       21-60|   491 1st Ave|Cafe Deli cious|New York|\n",
      "|sg:0e86fc3cfbc141...|2020-03-02|       21-60|   491 1st Ave|Cafe Deli cious|New York|\n",
      "+--------------------+----------+------------+--------------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfK = dfJ.join(dfG, 'safegraph_place_id')\n",
    "dfK.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cib3O6k38pEP"
   },
   "source": [
    "### L. Next, we need to collapse the two week data for each restaurant into a map (keys are for the week label).\n",
    "\n",
    "This can be done by \"grouping\" records by `safegraph_place_id`, then for each group: turn a list of `median_dwell` into a map using `map_from_arrays` while keeping any value in the other columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NRJrh14IDGiC",
    "outputId": "84eefc51-750c-4353-e2cd-1c30929d9d29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------+--------+--------------------+\n",
      "|  safegraph_place_id|  location_name|street_address|    city|              dwells|\n",
      "+--------------------+---------------+--------------+--------+--------------------+\n",
      "|sg:0e86fc3cfbc141...|Cafe Deli cious|   491 1st Ave|New York|{2020-03-30 -> 21...|\n",
      "+--------------------+---------------+--------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfL = dfK.groupBy('safegraph_place_id') \\\n",
    "    .agg(F.first('location_name').alias('location_name'), \n",
    "         F.first('street_address').alias('street_address'),\n",
    "         F.first('city').alias('city'), \n",
    "         F.map_from_arrays(F.collect_list('date'), \n",
    "                           F.collect_list('median_dwell')).alias('dwells'))\n",
    "\n",
    "dfL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QusMCXArbRZ9"
   },
   "source": [
    "### M. Before we get the expected output, we have to expand the `dwells` column into two columns, one for each month. Then, we can safely drop the two columns `safegraph_place_id` and `dwells`, and replace any N/A value with the string 'N/A'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3KmkWP_bRaI",
    "outputId": "41aef571-f73b-43fd-9b91-d4520188c325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------+-----+-----+\n",
      "|  location_name|street_address|    city|March|April|\n",
      "+---------------+--------------+--------+-----+-----+\n",
      "|Cafe Deli cious|   491 1st Ave|New York|21-60|21-60|\n",
      "+---------------+--------------+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfM = dfL \\\n",
    "    .withColumn('March', dfL.dwells[datetime.date(2020,3,2)]) \\\n",
    "    .withColumn('April', dfL.dwells[datetime.date(2020,3,30)]) \\\n",
    "    .drop('safegraph_place_id', 'dwells') \\\n",
    "    .fillna('N/A', ['March', 'April'])\n",
    "dfM.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmIrtbK8Ev2F"
   },
   "source": [
    "### N. Finally, we can join all the columns and prepare for our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hK2PY0BXEv2R",
    "outputId": "1cf1a0b0-f321-475a-9e2a-9ce0a7c7e8d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cafe Deli cious,491 1st Ave,New York,21-60,21-60\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(sorted(dfM.rdd.map(lambda x: ','.join(x)).collect())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dN0ikTZtIEta"
   },
   "source": [
    "### O. Putting it all together, replace `dfPattern` with the full data set and rerun your code from the above steps to get the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFH4FEk13-aW",
    "outputId": "82e73657-9f61-441c-96ef-de1be7df3aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 In 1 Kitchen,4902 Fort Hamilton Pkwy,Brooklyn,21-60,21-60\n",
      "Agape Cafe,655 W 34th St,New York,21-60,21-60\n",
      "Buffalo Wild Wings,632 Gateway Dr,Brooklyn,21-60,11-20\n",
      "Burger King,2800 Hylan Blvd,Staten Island,11-20,11-20\n",
      "Cafe Deli cious,491 1st Ave,New York,21-60,21-60\n",
      "Cinnabon,1313 Broadway,New York,11-20,11-20\n",
      "Dunkin',150 B Greaves Laneevergreen Plaza,New York,11-20,21-60\n",
      "Dunkin',1752 Shore Pkwybjs Wholesale Club,New York,21-60,21-60\n",
      "Dunkin',2449 Veterans Rd Wshop Rite,New York,21-60,21-60\n",
      "Dunkin',590 Gateway Dr,Brooklyn,21-60,21-60\n",
      "Dunkin',625 Atlantic Aveatlantic Center Mall,New York,21-60,21-60\n",
      "Dunkin',6620 Avenue U,Brooklyn,11-20,11-20\n",
      "Food Express Truck,2501 Forest Ave Across From Home Depot,New York,11-20,21-60\n",
      "Golden Krust Caribbean Bakery and Grill,1364 Pennsylvania Ave,Brooklyn,11-20,11-20\n",
      "Harlem Tavern,2153 Frederick Douglass Blvd,New York,11-20,11-20\n",
      "Hutong,731 Lexington Ave,New York,21-60,21-60\n",
      "Khan Express,1275 York Ave,New York,21-60,21-60\n",
      "King Cab Halal Food,10th Ave & 28th St,New York,N/A,21-60\n",
      "McDonald's,1403 Mermaid Ave,Brooklyn,5-10,5-10\n",
      "McDonald's,1600 Bruckner Blvd,Bronx,5-10,11-20\n",
      "McDonald's,2154 Hylan Blvd,Staten Island,11-20,5-10\n",
      "McDonald's,260 Page Ave,Staten Island,11-20,5-10\n",
      "Ninja 86 Sushi,2274 86th St,Brooklyn,11-20,11-20\n",
      "PROOF Coffee Roasters,335 E 27th St,New York,21-60,21-60\n",
      "Pizza Gusta,2945 Bruckner Blvd,Bronx,11-20,21-60\n",
      "Plaza Cafeteria Mount Sinai Hospital,1428 Madison Ave,New York,21-60,21-60\n",
      "Red Mango,234 W 34th St,New York,11-20,11-20\n",
      "Roti R Us,1493 Albany Ave,Brooklyn,21-60,21-60\n",
      "Starbucks,655 W 34th St,New York,11-20,21-60\n",
      "The Dumplin Shop,3852 Bronxwood Ave,Bronx,21-60,21-60\n"
     ]
    }
   ],
   "source": [
    "dfPattern = spark.read.csv('nyc_restaurant_pattern.csv', \n",
    "                           header=True, escape='\"')\n",
    "\n",
    "dfG = dfPattern.select('safegraph_place_id', 'date_range_start', 'visits_by_day', 'location_name', 'street_address', 'city')\n",
    "\n",
    "\n",
    "def expandVisits(date_range_start, visits_by_day):\n",
    "    '''\n",
    "    This function needs to return a Python's dict{datetime:int} where:\n",
    "      key   : datetime type, e.g. datetime(2020,3,17), etc.\n",
    "      value : the number of visits for that day\n",
    "    '''\n",
    "    date = date_range_start.split('T')[0].split('-')\n",
    "    year = int(date[0])\n",
    "    month = int(date[1])\n",
    "    day = int(date[2])\n",
    "    vbd = visits_by_day[1:-1].split(',')\n",
    "    mydict = dict()\n",
    "\n",
    "    week_day_count = 0\n",
    "    for i in vbd:\n",
    "      mydict[datetime.date(year, month, day)+datetime.timedelta(days=week_day_count)] = int(i)\n",
    "      week_day_count += 1\n",
    "    \n",
    "    return mydict\n",
    "    \n",
    "\n",
    "udfExpand = F.udf(expandVisits, MapType(DateType(), IntegerType()))\n",
    "dfG = dfG.select('safegraph_place_id',\n",
    "                 F.explode(udfExpand('date_range_start', 'visits_by_day')) \\\n",
    "                    .alias('date', 'visits'), \\\n",
    "                 'location_name', 'street_address', 'city')\n",
    "\n",
    "dfG = dfG.filter(dfG[\"date\"] >= datetime.date(2020,4,1))\n",
    "\n",
    "dfG = dfG.groupBy('safegraph_place_id').agg({'visits':'max', 'location_name':'first', 'street_address':'first', 'city':'first'}) \\\n",
    "        .withColumnRenamed('max(visits)', 'max_visits') \\\n",
    "        .withColumnRenamed('first(location_name)', 'location_name') \\\n",
    "        .withColumnRenamed('first(street_address)', 'street_address') \\\n",
    "        .withColumnRenamed('first(city)', 'city')\n",
    "\n",
    "      \n",
    "dfG = dfG.filter(dfG.max_visits >= 50)\n",
    "\n",
    "dfG = dfG.drop('max_visits')\n",
    "\n",
    "\n",
    "dfH = dfPattern.select('safegraph_place_id', 'date_range_start', 'bucketed_dwell_times')\n",
    "dfH = dfH.withColumn(\"date_range_start\", F.split(dfH.date_range_start, 'T').getItem(0)).withColumnRenamed('date_range_start', 'date')\n",
    "dfH = dfH.filter((dfH.date == '2020-03-02') | (dfH.date == '2020-03-30'))\n",
    "dfH = dfH.withColumn(\"date\", F.to_date(dfH.date, 'yyyy-MM-dd'))\n",
    "\n",
    "def medianDwellBucket(dwells):\n",
    "  dwells = json.loads(dwells)\n",
    "  del dwells['>240']\n",
    "  total = sum(dwells.values())\n",
    "  middle = total/2\n",
    "  tmp = 0\n",
    "  for key in dwells:\n",
    "    tmp = tmp + dwells[key]\n",
    "\n",
    "    if tmp > middle:\n",
    "      return key\n",
    "\n",
    "udfMedian = F.udf(lambda x: medianDwellBucket(x), StringType())\n",
    "\n",
    "dfJ = dfH.withColumn('median_dwell', udfMedian('bucketed_dwell_times')) \\\n",
    "    .select('safegraph_place_id', 'date', 'median_dwell')\n",
    "\n",
    "dfK = dfJ.join(dfG, 'safegraph_place_id')\n",
    "dfL = dfK.groupBy('safegraph_place_id') \\\n",
    "    .agg(F.first('location_name').alias('location_name'), \n",
    "         F.first('street_address').alias('street_address'),\n",
    "         F.first('city').alias('city'), \n",
    "         F.map_from_arrays(F.collect_list('date'), \n",
    "                           F.collect_list('median_dwell')).alias('dwells'))\n",
    "    \n",
    "dfM = dfL \\\n",
    "    .withColumn('March', dfL.dwells[datetime.date(2020,3,2)]) \\\n",
    "    .withColumn('April', dfL.dwells[datetime.date(2020,3,30)]) \\\n",
    "    .drop('safegraph_place_id', 'dwells') \\\n",
    "    .fillna('N/A', ['March', 'April'])\n",
    "\n",
    "\n",
    "print('\\n'.join(sorted(dfM.rdd.map(lambda x: ','.join(x)).collect())))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4D1Sn_NiL9Ls",
    "LcJgwfFuirqN"
   ],
   "name": "BDM_HW3_Walkthrough_DataFrame_Lin",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
